{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Logistic_Regression_Intro_to_Deep_Learning.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Y8aACPv9xYg"
      },
      "source": [
        "# Logistic Regression\n",
        "## Intro to Deep Learning Class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kXb8z-EkuARS"
      },
      "source": [
        "## Importing Libraries\n",
        "Before we start, let's import some necessary libraries we will be using."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NR89YDDUvhce",
        "outputId": "61bf92d3-e41d-48fb-858c-34df3605923e"
      },
      "source": [
        "import sklearn # Sklearn\n",
        "!pip install -U scikit-learn\n",
        "print(\"\\n\\nSklearn version: \" + str(sklearn.__version__))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (1.0.1)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.19.5)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.1.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.4.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (3.0.0)\n",
            "\n",
            "\n",
            "Sklearn version: 1.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hBKcmplftezQ"
      },
      "source": [
        "import numpy as np # NumPy\n",
        "import pandas as pd # Pandas\n",
        "import matplotlib.pyplot as plt # Matplotlib\n",
        "from IPython.display import display # For displaying the Pandas DataFrame\n",
        "import math # Math library\n",
        "from sklearn.datasets import load_breast_cancer # Import data set from sklearn\n",
        "from sklearn.model_selection import train_test_split # For splitting the data set into train and test sets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2XiILZAy4Go"
      },
      "source": [
        "## Import Data set from Sklearn\n",
        "We will be using a data set on benign (harmless) and malignant (harmful) breast tumors. In this data set, we are given some information about the tumor and the corresponding diagnosis. \n",
        "\n",
        "In this notebook, we will use Logistic Regression to predict whether a tumor is benign or malignant (label) given information about the tumor (features).\n",
        "\n",
        "In the following code boxes, we import the data from Sklearn."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rL7ASTj6t_oN"
      },
      "source": [
        "data = load_breast_cancer(return_X_y=True, as_frame=True) # Import data set from sklearn and store it in \"data\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oWqkQeMEuQrP"
      },
      "source": [
        "X, y = data # \"data\" is a tuple: (X,y). This line stores the x-values in \"X\" and the y=values in \"y\".\n",
        "y = pd.DataFrame({\"diagnosis\": y}) # Convert y to a Pandas DataFrame\n",
        "\n",
        "# Store NumPy equivalents of the X and y variables\n",
        "X_np = X.values\n",
        "y_np = y.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYEJEw_b--s2"
      },
      "source": [
        "### Quick-Look\n",
        "In the following code box, we take a quick look at both the features and the labels. Note that while the $X$ and $y$ values are Pandas Data Frames, you do not need to know about these data types for this assignment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        },
        "id": "exwckHOAuXhG",
        "outputId": "c982712b-b53e-44de-c375-26758b1464ef"
      },
      "source": [
        "print(\"X-Values (Features): \")\n",
        "display(X.head())\n",
        "print(\"\\n\\nY-Value (Label): \")\n",
        "display(y.head())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X-Values (Features): \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>mean radius</th>\n",
              "      <th>mean texture</th>\n",
              "      <th>mean perimeter</th>\n",
              "      <th>mean area</th>\n",
              "      <th>mean smoothness</th>\n",
              "      <th>mean compactness</th>\n",
              "      <th>mean concavity</th>\n",
              "      <th>mean concave points</th>\n",
              "      <th>mean symmetry</th>\n",
              "      <th>mean fractal dimension</th>\n",
              "      <th>radius error</th>\n",
              "      <th>texture error</th>\n",
              "      <th>perimeter error</th>\n",
              "      <th>area error</th>\n",
              "      <th>smoothness error</th>\n",
              "      <th>compactness error</th>\n",
              "      <th>concavity error</th>\n",
              "      <th>concave points error</th>\n",
              "      <th>symmetry error</th>\n",
              "      <th>fractal dimension error</th>\n",
              "      <th>worst radius</th>\n",
              "      <th>worst texture</th>\n",
              "      <th>worst perimeter</th>\n",
              "      <th>worst area</th>\n",
              "      <th>worst smoothness</th>\n",
              "      <th>worst compactness</th>\n",
              "      <th>worst concavity</th>\n",
              "      <th>worst concave points</th>\n",
              "      <th>worst symmetry</th>\n",
              "      <th>worst fractal dimension</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>17.99</td>\n",
              "      <td>10.38</td>\n",
              "      <td>122.80</td>\n",
              "      <td>1001.0</td>\n",
              "      <td>0.11840</td>\n",
              "      <td>0.27760</td>\n",
              "      <td>0.3001</td>\n",
              "      <td>0.14710</td>\n",
              "      <td>0.2419</td>\n",
              "      <td>0.07871</td>\n",
              "      <td>1.0950</td>\n",
              "      <td>0.9053</td>\n",
              "      <td>8.589</td>\n",
              "      <td>153.40</td>\n",
              "      <td>0.006399</td>\n",
              "      <td>0.04904</td>\n",
              "      <td>0.05373</td>\n",
              "      <td>0.01587</td>\n",
              "      <td>0.03003</td>\n",
              "      <td>0.006193</td>\n",
              "      <td>25.38</td>\n",
              "      <td>17.33</td>\n",
              "      <td>184.60</td>\n",
              "      <td>2019.0</td>\n",
              "      <td>0.1622</td>\n",
              "      <td>0.6656</td>\n",
              "      <td>0.7119</td>\n",
              "      <td>0.2654</td>\n",
              "      <td>0.4601</td>\n",
              "      <td>0.11890</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>20.57</td>\n",
              "      <td>17.77</td>\n",
              "      <td>132.90</td>\n",
              "      <td>1326.0</td>\n",
              "      <td>0.08474</td>\n",
              "      <td>0.07864</td>\n",
              "      <td>0.0869</td>\n",
              "      <td>0.07017</td>\n",
              "      <td>0.1812</td>\n",
              "      <td>0.05667</td>\n",
              "      <td>0.5435</td>\n",
              "      <td>0.7339</td>\n",
              "      <td>3.398</td>\n",
              "      <td>74.08</td>\n",
              "      <td>0.005225</td>\n",
              "      <td>0.01308</td>\n",
              "      <td>0.01860</td>\n",
              "      <td>0.01340</td>\n",
              "      <td>0.01389</td>\n",
              "      <td>0.003532</td>\n",
              "      <td>24.99</td>\n",
              "      <td>23.41</td>\n",
              "      <td>158.80</td>\n",
              "      <td>1956.0</td>\n",
              "      <td>0.1238</td>\n",
              "      <td>0.1866</td>\n",
              "      <td>0.2416</td>\n",
              "      <td>0.1860</td>\n",
              "      <td>0.2750</td>\n",
              "      <td>0.08902</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>19.69</td>\n",
              "      <td>21.25</td>\n",
              "      <td>130.00</td>\n",
              "      <td>1203.0</td>\n",
              "      <td>0.10960</td>\n",
              "      <td>0.15990</td>\n",
              "      <td>0.1974</td>\n",
              "      <td>0.12790</td>\n",
              "      <td>0.2069</td>\n",
              "      <td>0.05999</td>\n",
              "      <td>0.7456</td>\n",
              "      <td>0.7869</td>\n",
              "      <td>4.585</td>\n",
              "      <td>94.03</td>\n",
              "      <td>0.006150</td>\n",
              "      <td>0.04006</td>\n",
              "      <td>0.03832</td>\n",
              "      <td>0.02058</td>\n",
              "      <td>0.02250</td>\n",
              "      <td>0.004571</td>\n",
              "      <td>23.57</td>\n",
              "      <td>25.53</td>\n",
              "      <td>152.50</td>\n",
              "      <td>1709.0</td>\n",
              "      <td>0.1444</td>\n",
              "      <td>0.4245</td>\n",
              "      <td>0.4504</td>\n",
              "      <td>0.2430</td>\n",
              "      <td>0.3613</td>\n",
              "      <td>0.08758</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>11.42</td>\n",
              "      <td>20.38</td>\n",
              "      <td>77.58</td>\n",
              "      <td>386.1</td>\n",
              "      <td>0.14250</td>\n",
              "      <td>0.28390</td>\n",
              "      <td>0.2414</td>\n",
              "      <td>0.10520</td>\n",
              "      <td>0.2597</td>\n",
              "      <td>0.09744</td>\n",
              "      <td>0.4956</td>\n",
              "      <td>1.1560</td>\n",
              "      <td>3.445</td>\n",
              "      <td>27.23</td>\n",
              "      <td>0.009110</td>\n",
              "      <td>0.07458</td>\n",
              "      <td>0.05661</td>\n",
              "      <td>0.01867</td>\n",
              "      <td>0.05963</td>\n",
              "      <td>0.009208</td>\n",
              "      <td>14.91</td>\n",
              "      <td>26.50</td>\n",
              "      <td>98.87</td>\n",
              "      <td>567.7</td>\n",
              "      <td>0.2098</td>\n",
              "      <td>0.8663</td>\n",
              "      <td>0.6869</td>\n",
              "      <td>0.2575</td>\n",
              "      <td>0.6638</td>\n",
              "      <td>0.17300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>20.29</td>\n",
              "      <td>14.34</td>\n",
              "      <td>135.10</td>\n",
              "      <td>1297.0</td>\n",
              "      <td>0.10030</td>\n",
              "      <td>0.13280</td>\n",
              "      <td>0.1980</td>\n",
              "      <td>0.10430</td>\n",
              "      <td>0.1809</td>\n",
              "      <td>0.05883</td>\n",
              "      <td>0.7572</td>\n",
              "      <td>0.7813</td>\n",
              "      <td>5.438</td>\n",
              "      <td>94.44</td>\n",
              "      <td>0.011490</td>\n",
              "      <td>0.02461</td>\n",
              "      <td>0.05688</td>\n",
              "      <td>0.01885</td>\n",
              "      <td>0.01756</td>\n",
              "      <td>0.005115</td>\n",
              "      <td>22.54</td>\n",
              "      <td>16.67</td>\n",
              "      <td>152.20</td>\n",
              "      <td>1575.0</td>\n",
              "      <td>0.1374</td>\n",
              "      <td>0.2050</td>\n",
              "      <td>0.4000</td>\n",
              "      <td>0.1625</td>\n",
              "      <td>0.2364</td>\n",
              "      <td>0.07678</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   mean radius  mean texture  ...  worst symmetry  worst fractal dimension\n",
              "0        17.99         10.38  ...          0.4601                  0.11890\n",
              "1        20.57         17.77  ...          0.2750                  0.08902\n",
              "2        19.69         21.25  ...          0.3613                  0.08758\n",
              "3        11.42         20.38  ...          0.6638                  0.17300\n",
              "4        20.29         14.34  ...          0.2364                  0.07678\n",
              "\n",
              "[5 rows x 30 columns]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Y-Value (Label): \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>diagnosis</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   diagnosis\n",
              "0          0\n",
              "1          0\n",
              "2          0\n",
              "3          0\n",
              "4          0"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wvo-vBXwx1Gs"
      },
      "source": [
        "## Data set Features\n",
        "Here is some information about the features and labels of our data set. This information is from [Sklearn](https://scikit-learn.org/stable/datasets/toy_dataset.html#breast-cancer-dataset), the provider of this data.\n",
        "\n",
        "**X-Values (Features)**\n",
        "- Radius: Mean of distances from center to points on the perimeter\n",
        "- Texture: Standard deviation of gray-scale values\n",
        "- Perimeter\n",
        "- Area\n",
        "- Smoothness: Local variation in radius lengths\n",
        "- Compactness: Calculated as (perimeter^2 / area - 1.0)\n",
        "- Concavity: Severity of concave portions of the contour\n",
        "- Concave Points: Number of concave portions of the contour\n",
        "- Symmetry\n",
        "- Fractal Dimension: (“coastline approximation” - 1)\n",
        "- The mean, standard error, and “worst” or largest (mean of the three worst/largest values) of these features were computed for each image, resulting in 30 features. For instance, field 0 is Mean Radius, field 10 is Radius SE, field 20 is Worst Radius.\n",
        "\n",
        "**Y-Value (Label)**\n",
        "- Label: Diagnosis (Benign or Malignant)\n",
        "\n",
        "**Additional Information**\n",
        "- Number of Examples: 569\n",
        "- Number of examples for Benign: 357\n",
        "- Number of examples for Malignant: 212\n",
        "- Features: 30"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aRF-_hlH3eLW",
        "outputId": "c73ba69b-4d2b-45d0-9905-00d52ad29657"
      },
      "source": [
        "m = X.shape[0]\n",
        "n = X.shape[1]\n",
        "print(\"Number of examples: \" + str(m))\n",
        "print(\"Number of features: \" + str(n))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of examples: 569\n",
            "Number of features: 30\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YreZ6LzP_n9I"
      },
      "source": [
        "## Data set Shape\n",
        "In the last code box, we found that our data set has 569 examples and 30 features. This kind of familiarity with the data set is always nice to know before creating the Machine Learning model. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n61cKWkx2Rp5"
      },
      "source": [
        "## Separating Train and Test Sets\n",
        "There are 569 examples in our data set. We will use about 80% of this data for training our Linear Regression model, and the remaining 20% of the data to evaluate our model. \n",
        "\n",
        "In the following code box, we split the data set into train and test sets. We will also keep NumPy array versions of the $X$ and $y$ values for training our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uN0ToM_S4nEO",
        "outputId": "03075c44-7ac1-4471-bab3-d759c325511f"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=0)\n",
        "\n",
        "# For the NumPy versions of the variables, we transpose them so that the examples are in the columns\n",
        "X_train_np = X_train.values.T\n",
        "X_test_np = X_test.values.T\n",
        "y_train_np = y_train.values.T\n",
        "y_test_np = y_test.values.T\n",
        "\n",
        "m_train = X_train.shape[0]\n",
        "m_test = X_test.shape[0]\n",
        "\n",
        "print(\"Number of examples in train set: \" + str(m_train))\n",
        "print(\"Number of examples in test set:\\t \" + str(m_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of examples in train set: 455\n",
            "Number of examples in test set:\t 114\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DN4NjVCCmzqQ"
      },
      "source": [
        "## Normalization\n",
        "For Linear Regression, we must **normalize** our values. In this case, that means bringing the mean to 0 and the standard deviation to 1 using the following code box. Note that we only do this for the X-Values (Features)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TdphP6yDnHi6"
      },
      "source": [
        "# X_train_np normalization\n",
        "X_train_np_mean = np.mean(X_train_np, axis=0)\n",
        "X_train_np_std = np.std(X_train_np, axis=0)\n",
        "X_train_np_norm = (X_train_np - X_train_np_mean)/X_train_np_std\n",
        "\n",
        "# X_test_np normalization\n",
        "X_test_np_mean = np.mean(X_test_np, axis=0)\n",
        "X_test_np_std = np.std(X_test_np, axis=0)\n",
        "X_test_np_norm = (X_test_np - X_test_np_mean)/X_test_np_std"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9ZYV-sZ0SXG"
      },
      "source": [
        "## Gradient Descent\n",
        "Now we will move onto the Logistic Regression model. For Logistic Regression, we use Gradient Descent. These are the steps of Gradient Descent (the parameter initialization is specific to Logistic Regression, but most of these steps are applied to (mostly) all types of Machine Learning and Deep Learning algorithms): \n",
        "1. Initialize Parameters\n",
        "> For Logistic Regression, we initialize all parameters to 0.\n",
        "2. Hypothesis Function\n",
        "> Use the paramters and input features to compute the hypothesis $\\hat{y}$. \n",
        "3. Cost Function\n",
        "> Use the prediction $\\hat{y}$ and the real label $y$ to compute the Binary Cross Entropy (Cross Entropy) cost function.\n",
        "4. Parameter Update\n",
        "> Compute the parameter update for all parameters and update them using the appropriate learning rate $\\alpha$ (alpha).\n",
        "5. Repeat!\n",
        "> Repeat Steps 2-4 many times until the cost is sufficiently low.\n",
        "\n",
        "In the following code boxes, we will write code for each of these steps. In the end, we will put these together to train the Logistic Regression model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wMBuahTx5fPT"
      },
      "source": [
        "### 1. Initialize Parameters\n",
        "Since there are 30 features, we will have 31 parameters (weights). \n",
        "\n",
        "We will initialize 30 of these to 0 and store them in $W$. Remember, $W$ should be a row vector, so its shape should be $(1,30)$.\n",
        "\n",
        "There is also 1 bias parameter which we will call $b$. We will also initialize this to 0.\n",
        "\n",
        "Therefore, there are 31 parameters total. In the following code box, we create a function that initializes the parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1J-DaXe2CPh"
      },
      "source": [
        "def initialize_parameters():\n",
        "  W = np.zeros((1,30))\n",
        "  b = 0\n",
        "  return W, b"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZbKQoWSg6-VI"
      },
      "source": [
        "### 2. Hypothesis Function\n",
        "Recall that the hypothesis function for Logistic Regression is:\n",
        "$$\\hat{\\textbf{Y}}=h(\\textbf{X},b,\\textbf{W})=\\sigma(\\textbf{W}\\times \\textbf{X} + b),$$\n",
        "where $\\times$ represents matrix multiplication. First, we create the sigmoid function. Mathematically, this function is:\n",
        "$$\\sigma(x) = \\frac{1}{1+e^{-x}}.$$\n",
        "In Python code, this is:\n",
        "```\n",
        "def sigmoid(x):\n",
        "  return 1/(1+e**(-x))\n",
        "```\n",
        "Then, we use this function to create the hypothesis function. In Python code, the hypothesis function would be:\n",
        "```\n",
        "y_hat = sigmoid(np.dot(W, X) + b)\n",
        "```\n",
        "In the next code box, we create the sigmoid function. In the code box after that, we implement the hypothesis function.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bTE8YhdbHyM0"
      },
      "source": [
        "def sigmoid(x):\n",
        "  return 1/(1+math.e**(-x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z53op74b692c"
      },
      "source": [
        "def hypothesis(X, b, W):\n",
        "  y_hat = sigmoid(np.dot(W, X) + b)\n",
        "  return y_hat"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6fzCWODTx_1"
      },
      "source": [
        "### 3. Cost Function\n",
        "Now, we define the cost function using $\\hat{\\textbf{Y}}$ and $\\textbf{Y}$. Remember, when $\\hat{\\textbf{Y}}$ and $\\textbf{Y}$ are matrices, the cost function is defined as follows:\n",
        "$$J(\\textbf{X},b,\\textbf{W})=-\\frac{1}{m}\\left[\\textbf{Y}\\log{\\hat{\\textbf{Y}}}+(1-\\textbf{Y})\\log{(1-\\hat{\\textbf{Y}})}\\right],$$\n",
        "where ''$-$'' and the squaring operation represent element-wise subtraction and squaring. Note that \"log\" is the natural log function with base $e$. In Python code, this would be:\n",
        "```\n",
        "J = - 1/(m) * (y*np.log(y_hat) + (1-y)*np.log(1-y_hat))\n",
        "```\n",
        "(Sidenote: For the Python function, instead of passing in $\\textbf{X}$, $b$, and $\\textbf{W}$, we will simply pass in $\\hat{\\textbf{Y}}$ (which we computed in the last step). Additionally, we will pass in $\\textbf{Y}$ and $m$.) \n",
        "\n",
        "In the next code box, we create the cost function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pi_Qp5aX65U0"
      },
      "source": [
        "def cost(y_hat, y, m):\n",
        "  J = - 1/m * np.sum(y*np.log(y_hat) + (1-y)*np.log(1-y_hat))\n",
        "  return J"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1vN90SqWDGx"
      },
      "source": [
        "### 4. Parameter Update\n",
        "After computing the cost, we have to adjust our parameters. Recall that the parameter update for $\\textbf{W}$ is:\n",
        "$$W:=W-\\alpha\\cdot \\frac{dJ}{d\\textbf{W}}.$$\n",
        "Similarly, this is how we update $b$:\n",
        "$$b:=b-\\alpha\\cdot \\frac{dJ}{db}.$$\n",
        "Additionally, here are the values for $\\frac{dJ}{d\\textbf{W}}$ and $\\frac{dJ}{db}$:\n",
        "\n",
        "\\begin{align*}\n",
        "\\frac{dJ}{d\\textbf{W}}&=-\\frac{1}{m}\\cdot \\left[\\frac{\\textbf{Y}}{\\hat{\\textbf{Y}}}+\\frac{1-\\textbf{Y}}{1-\\hat{\\textbf{Y}}}\\right]\\cdot \\hat{\\textbf{Y}}\\cdot (1-\\hat{\\textbf{Y}})\\times \\textbf{X}^T\\\\\n",
        "\\frac{dJ}{db}&=-\\frac{1}{m}\\cdot \\text{sum}\\left[\\left(\\frac{\\textbf{Y}}{\\hat{\\textbf{Y}}}+\\frac{1-\\textbf{Y}}{1-\\hat{\\textbf{Y}}}\\right)\\cdot \\hat{\\textbf{Y}}\\cdot (1-\\hat{\\textbf{Y}})\\right]\n",
        "\\end{align*}\n",
        "\n",
        "In Python code, these 4 equations would be:\n",
        "```\n",
        "W = W - alpha * dJdW\n",
        "b = b - alpha * dJdb\n",
        "dJdW = - 1/m * np.dot((y*y_hat + (1-y)*(1-y_hat)) * y_hat * (1 - y_hat), X.T)\n",
        "dJdb = - 1/m * np.sum((y*y_hat + (1-y)*(1-y_hat)) * y_hat * (1 - y_hat))\n",
        "```\n",
        "Note that in Python, we will run the last two lines of codes first (otherwise we will get an error). In the next code box, we create a function to update our parameters. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8kMIa6y7WBxe"
      },
      "source": [
        "def parameter_update(X, y_hat, y, m, alpha, W, b):\n",
        "  dJdW = - 1/m * np.dot((y*y_hat + (1-y)*(1-y_hat)) * y_hat * (1 - y_hat), X.T)\n",
        "  dJdb = - 1/m * np.sum((y*y_hat + (1-y)*(1-y_hat)) * y_hat * (1 - y_hat))\n",
        "\n",
        "  W = W - alpha * dJdW\n",
        "  b = b - alpha * dJdb\n",
        "\n",
        "  return W, b"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5QJ3XB7naBVb"
      },
      "source": [
        "### Put it all together!\n",
        "We will now do Step 5 of Gradient Descent (Repeat!) by putting all of the previous functions together into a model function. This is what we will do in this function:\n",
        "1. Initialize Parameters\n",
        "\n",
        "For as many iterations as we chose to train our model:\n",
        "2. Compute Hypothesis\n",
        "3. Compute Cost Function\n",
        "4. Update Parameters\n",
        "\n",
        "Finally, we return the learned parameters $\\textbf{W}$ and $b$. While training the model, we will print the cost on the train set every few iterations and, along with this, the cost on the test set. We will keep track of these costs and return some values that are helpful in graphing our performance.\n",
        "\n",
        "In the following code box, we create the model function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ds0BRYr1aAvw"
      },
      "source": [
        "def model(X_train, y_train, m_train, X_test, y_test, m_test, alpha=0.1, iterations=1000):\n",
        "  # Initialize Parameters\n",
        "  W, b = initialize_parameters()\n",
        "\n",
        "  # Keep track of costs for train and test sets\n",
        "  iteration_nums = [] # X-value for performance graph\n",
        "  costs_train = []\n",
        "  costs_test = []\n",
        "\n",
        "  # Repeat the following steps of Gradient Descent using a for-loop\n",
        "  for i in range(iterations):\n",
        "    # Compute Hypothesis\n",
        "    y_hat = hypothesis(X_train, b, W)\n",
        "\n",
        "    # Compute Cost Function for train set and test set and print this out\n",
        "    # Only do this once every 100 iterations\n",
        "    if i % 100 == 0:\n",
        "      iteration_nums.append(i) # Add iteration number to list of iteration numbers\n",
        "      cost_train = cost(y_hat, y_train, m_train) # Compute cost for train set\n",
        "      costs_train.append(cost_train) # Add cost to list of train costs\n",
        "      y_hat_test = hypothesis(X_test, b, W) # Compute hypothesis for test set\n",
        "      cost_test = cost(y_hat_test, y_test, m_test) # Compute cost for test set\n",
        "      costs_test.append(cost_test) # Add cost to list of test costs\n",
        "      print(\"\\n\\nIteration \" + str(i))\n",
        "      print(\"Train set cost: \" + str(cost_train))\n",
        "      print(\"Test set cost: \" + str(cost_test))\n",
        "    \n",
        "    # Update Parameters\n",
        "    W, b = parameter_update(X_train, y_hat, y_train, m_train, alpha, W, b)\n",
        "  \n",
        "  # Print final costs for train and test sets\n",
        "  iteration_nums.append(iterations) # Add iteration number to list of iteration numbers\n",
        "  y_hat = hypothesis(X_train, b, W) # Compute hypothesis for train set\n",
        "  cost_train = cost(y_hat, y_train, m_train) # Compute cost for train set\n",
        "  costs_train.append(cost_train) # Add cost to list of train costs\n",
        "  y_hat_test = hypothesis(X_test, b, W) # Compute hypothesis for test set\n",
        "  cost_test = cost(y_hat_test, y_test, m_test) # Compute cost for test set\n",
        "  costs_test.append(cost_test) # Add cost to list of test costs\n",
        "  print(\"\\n\\n\")\n",
        "  print(\"Final train set cost: \" + str(cost_train))\n",
        "  print(\"Final test set cost: \" + str(cost_test))\n",
        "\n",
        "  # Return learned parameters\n",
        "  return W, b, iteration_nums, costs_train, costs_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xnX2ogahvbr"
      },
      "source": [
        "## Training the Model\n",
        "Finally, we get to train the Logistic Regression model. In the next code box, we run the model function with the appropriate arguments and store the learned parameters in the variables $\\textbf{W}$ and $b$. Note that we feed in the NumPy version of all variables to avoid any errors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJEz-5cHaAJB",
        "outputId": "2dfbbcf9-39d9-4f61-8f53-148144df1e8b"
      },
      "source": [
        "W, b, iteration_nums, costs_train, costs_test = model(X_train=X_train_np_norm, y_train=y_train_np, m_train=m_train, X_test=X_test_np_norm, y_test=y_test_np, m_test=m_test, alpha=1e-5, iterations=10000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Iteration 0\n",
            "Train set cost: 0.6931471805599453\n",
            "Test set cost: 0.6931471805599453\n",
            "\n",
            "\n",
            "Iteration 100\n",
            "Train set cost: 0.6926168394928924\n",
            "Test set cost: 0.692808856963245\n",
            "\n",
            "\n",
            "Iteration 200\n",
            "Train set cost: 0.6920899270448324\n",
            "Test set cost: 0.6924740636550372\n",
            "\n",
            "\n",
            "Iteration 300\n",
            "Train set cost: 0.6915664527896891\n",
            "Test set cost: 0.6921428088307883\n",
            "\n",
            "\n",
            "Iteration 400\n",
            "Train set cost: 0.6910464261862582\n",
            "Test set cost: 0.691815100564864\n",
            "\n",
            "\n",
            "Iteration 500\n",
            "Train set cost: 0.6905298565771698\n",
            "Test set cost: 0.6914909468095597\n",
            "\n",
            "\n",
            "Iteration 600\n",
            "Train set cost: 0.6900167531878579\n",
            "Test set cost: 0.6911703553941437\n",
            "\n",
            "\n",
            "Iteration 700\n",
            "Train set cost: 0.6895071251255471\n",
            "Test set cost: 0.690853334023914\n",
            "\n",
            "\n",
            "Iteration 800\n",
            "Train set cost: 0.6890009813782448\n",
            "Test set cost: 0.6905398902792677\n",
            "\n",
            "\n",
            "Iteration 900\n",
            "Train set cost: 0.6884983308137522\n",
            "Test set cost: 0.6902300316147812\n",
            "\n",
            "\n",
            "Iteration 1000\n",
            "Train set cost: 0.6879991821786817\n",
            "Test set cost: 0.6899237653583078\n",
            "\n",
            "\n",
            "Iteration 1100\n",
            "Train set cost: 0.6875035440974908\n",
            "Test set cost: 0.689621098710084\n",
            "\n",
            "\n",
            "Iteration 1200\n",
            "Train set cost: 0.6870114250715268\n",
            "Test set cost: 0.6893220387418532\n",
            "\n",
            "\n",
            "Iteration 1300\n",
            "Train set cost: 0.6865228334780853\n",
            "Test set cost: 0.6890265923960013\n",
            "\n",
            "\n",
            "Iteration 1400\n",
            "Train set cost: 0.6860377775694817\n",
            "Test set cost: 0.6887347664847071\n",
            "\n",
            "\n",
            "Iteration 1500\n",
            "Train set cost: 0.6855562654721372\n",
            "Test set cost: 0.6884465676891065\n",
            "\n",
            "\n",
            "Iteration 1600\n",
            "Train set cost: 0.6850783051856763\n",
            "Test set cost: 0.6881620025584729\n",
            "\n",
            "\n",
            "Iteration 1700\n",
            "Train set cost: 0.6846039045820405\n",
            "Test set cost: 0.68788107750941\n",
            "\n",
            "\n",
            "Iteration 1800\n",
            "Train set cost: 0.6841330714046144\n",
            "Test set cost: 0.6876037988250605\n",
            "\n",
            "\n",
            "Iteration 1900\n",
            "Train set cost: 0.6836658132673691\n",
            "Test set cost: 0.6873301726543322\n",
            "\n",
            "\n",
            "Iteration 2000\n",
            "Train set cost: 0.6832021376540152\n",
            "Test set cost: 0.6870602050111347\n",
            "\n",
            "\n",
            "Iteration 2100\n",
            "Train set cost: 0.6827420519171764\n",
            "Test set cost: 0.6867939017736365\n",
            "\n",
            "\n",
            "Iteration 2200\n",
            "Train set cost: 0.6822855632775732\n",
            "Test set cost: 0.6865312686835355\n",
            "\n",
            "\n",
            "Iteration 2300\n",
            "Train set cost: 0.6818326788232251\n",
            "Test set cost: 0.6862723113453454\n",
            "\n",
            "\n",
            "Iteration 2400\n",
            "Train set cost: 0.6813834055086667\n",
            "Test set cost: 0.6860170352256989\n",
            "\n",
            "\n",
            "Iteration 2500\n",
            "Train set cost: 0.6809377501541801\n",
            "Test set cost: 0.6857654456526676\n",
            "\n",
            "\n",
            "Iteration 2600\n",
            "Train set cost: 0.6804957194450421\n",
            "Test set cost: 0.6855175478150971\n",
            "\n",
            "\n",
            "Iteration 2700\n",
            "Train set cost: 0.6800573199307891\n",
            "Test set cost: 0.6852733467619603\n",
            "\n",
            "\n",
            "Iteration 2800\n",
            "Train set cost: 0.6796225580244963\n",
            "Test set cost: 0.685032847401727\n",
            "\n",
            "\n",
            "Iteration 2900\n",
            "Train set cost: 0.6791914400020753\n",
            "Test set cost: 0.6847960545017485\n",
            "\n",
            "\n",
            "Iteration 3000\n",
            "Train set cost: 0.6787639720015857\n",
            "Test set cost: 0.6845629726876657\n",
            "\n",
            "\n",
            "Iteration 3100\n",
            "Train set cost: 0.6783401600225664\n",
            "Test set cost: 0.6843336064428254\n",
            "\n",
            "\n",
            "Iteration 3200\n",
            "Train set cost: 0.6779200099253815\n",
            "Test set cost: 0.6841079601077221\n",
            "\n",
            "\n",
            "Iteration 3300\n",
            "Train set cost: 0.6775035274305842\n",
            "Test set cost: 0.6838860378794537\n",
            "\n",
            "\n",
            "Iteration 3400\n",
            "Train set cost: 0.6770907181182985\n",
            "Test set cost: 0.6836678438111948\n",
            "\n",
            "\n",
            "Iteration 3500\n",
            "Train set cost: 0.6766815874276165\n",
            "Test set cost: 0.6834533818116912\n",
            "\n",
            "\n",
            "Iteration 3600\n",
            "Train set cost: 0.6762761406560154\n",
            "Test set cost: 0.6832426556447677\n",
            "\n",
            "\n",
            "Iteration 3700\n",
            "Train set cost: 0.6758743829587907\n",
            "Test set cost: 0.6830356689288581\n",
            "\n",
            "\n",
            "Iteration 3800\n",
            "Train set cost: 0.675476319348508\n",
            "Test set cost: 0.6828324251365521\n",
            "\n",
            "\n",
            "Iteration 3900\n",
            "Train set cost: 0.6750819546944726\n",
            "Test set cost: 0.6826329275941603\n",
            "\n",
            "\n",
            "Iteration 4000\n",
            "Train set cost: 0.6746912937222169\n",
            "Test set cost: 0.682437179481299\n",
            "\n",
            "\n",
            "Iteration 4100\n",
            "Train set cost: 0.6743043410130061\n",
            "Test set cost: 0.6822451838304933\n",
            "\n",
            "\n",
            "Iteration 4200\n",
            "Train set cost: 0.6739211010033634\n",
            "Test set cost: 0.6820569435267975\n",
            "\n",
            "\n",
            "Iteration 4300\n",
            "Train set cost: 0.6735415779846118\n",
            "Test set cost: 0.6818724613074368\n",
            "\n",
            "\n",
            "Iteration 4400\n",
            "Train set cost: 0.6731657761024362\n",
            "Test set cost: 0.681691739761468\n",
            "\n",
            "\n",
            "Iteration 4500\n",
            "Train set cost: 0.6727936993564637\n",
            "Test set cost: 0.6815147813294559\n",
            "\n",
            "\n",
            "Iteration 4600\n",
            "Train set cost: 0.6724253515998616\n",
            "Test set cost: 0.6813415883031746\n",
            "\n",
            "\n",
            "Iteration 4700\n",
            "Train set cost: 0.6720607365389557\n",
            "Test set cost: 0.6811721628253217\n",
            "\n",
            "\n",
            "Iteration 4800\n",
            "Train set cost: 0.6716998577328687\n",
            "Test set cost: 0.6810065068892583\n",
            "\n",
            "\n",
            "Iteration 4900\n",
            "Train set cost: 0.6713427185931741\n",
            "Test set cost: 0.6808446223387639\n",
            "\n",
            "\n",
            "Iteration 5000\n",
            "Train set cost: 0.6709893223835727\n",
            "Test set cost: 0.6806865108678117\n",
            "\n",
            "\n",
            "Iteration 5100\n",
            "Train set cost: 0.6706396722195871\n",
            "Test set cost: 0.680532174020366\n",
            "\n",
            "\n",
            "Iteration 5200\n",
            "Train set cost: 0.6702937710682759\n",
            "Test set cost: 0.6803816131901957\n",
            "\n",
            "\n",
            "Iteration 5300\n",
            "Train set cost: 0.6699516217479663\n",
            "Test set cost: 0.6802348296207109\n",
            "\n",
            "\n",
            "Iteration 5400\n",
            "Train set cost: 0.6696132269280075\n",
            "Test set cost: 0.680091824404816\n",
            "\n",
            "\n",
            "Iteration 5500\n",
            "Train set cost: 0.6692785891285431\n",
            "Test set cost: 0.6799525984847856\n",
            "\n",
            "\n",
            "Iteration 5600\n",
            "Train set cost: 0.6689477107203037\n",
            "Test set cost: 0.6798171526521587\n",
            "\n",
            "\n",
            "Iteration 5700\n",
            "Train set cost: 0.6686205939244177\n",
            "Test set cost: 0.6796854875476523\n",
            "\n",
            "\n",
            "Iteration 5800\n",
            "Train set cost: 0.6682972408122436\n",
            "Test set cost: 0.6795576036610966\n",
            "\n",
            "\n",
            "Iteration 5900\n",
            "Train set cost: 0.6679776533052204\n",
            "Test set cost: 0.6794335013313887\n",
            "\n",
            "\n",
            "Iteration 6000\n",
            "Train set cost: 0.6676618331747395\n",
            "Test set cost: 0.6793131807464662\n",
            "\n",
            "\n",
            "Iteration 6100\n",
            "Train set cost: 0.667349782042035\n",
            "Test set cost: 0.6791966419433028\n",
            "\n",
            "\n",
            "Iteration 6200\n",
            "Train set cost: 0.6670415013780937\n",
            "Test set cost: 0.6790838848079206\n",
            "\n",
            "\n",
            "Iteration 6300\n",
            "Train set cost: 0.6667369925035873\n",
            "Test set cost: 0.6789749090754251\n",
            "\n",
            "\n",
            "Iteration 6400\n",
            "Train set cost: 0.6664362565888209\n",
            "Test set cost: 0.678869714330059\n",
            "\n",
            "\n",
            "Iteration 6500\n",
            "Train set cost: 0.6661392946537045\n",
            "Test set cost: 0.6787683000052759\n",
            "\n",
            "\n",
            "Iteration 6600\n",
            "Train set cost: 0.6658461075677419\n",
            "Test set cost: 0.678670665383834\n",
            "\n",
            "\n",
            "Iteration 6700\n",
            "Train set cost: 0.6655566960500416\n",
            "Test set cost: 0.6785768095979109\n",
            "\n",
            "\n",
            "Iteration 6800\n",
            "Train set cost: 0.6652710606693465\n",
            "Test set cost: 0.6784867316292348\n",
            "\n",
            "\n",
            "Iteration 6900\n",
            "Train set cost: 0.6649892018440824\n",
            "Test set cost: 0.6784004303092399\n",
            "\n",
            "\n",
            "Iteration 7000\n",
            "Train set cost: 0.6647111198424285\n",
            "Test set cost: 0.6783179043192396\n",
            "\n",
            "\n",
            "Iteration 7100\n",
            "Train set cost: 0.6644368147824061\n",
            "Test set cost: 0.6782391521906176\n",
            "\n",
            "\n",
            "Iteration 7200\n",
            "Train set cost: 0.6641662866319878\n",
            "Test set cost: 0.6781641723050424\n",
            "\n",
            "\n",
            "Iteration 7300\n",
            "Train set cost: 0.6638995352092252\n",
            "Test set cost: 0.678092962894699\n",
            "\n",
            "\n",
            "Iteration 7400\n",
            "Train set cost: 0.6636365601823986\n",
            "Test set cost: 0.6780255220425394\n",
            "\n",
            "\n",
            "Iteration 7500\n",
            "Train set cost: 0.6633773610701835\n",
            "Test set cost: 0.6779618476825563\n",
            "\n",
            "\n",
            "Iteration 7600\n",
            "Train set cost: 0.6631219372418388\n",
            "Test set cost: 0.6779019376000714\n",
            "\n",
            "\n",
            "Iteration 7700\n",
            "Train set cost: 0.6628702879174133\n",
            "Test set cost: 0.6778457894320471\n",
            "\n",
            "\n",
            "Iteration 7800\n",
            "Train set cost: 0.6626224121679726\n",
            "Test set cost: 0.6777934006674153\n",
            "\n",
            "\n",
            "Iteration 7900\n",
            "Train set cost: 0.662378308915844\n",
            "Test set cost: 0.6777447686474256\n",
            "\n",
            "\n",
            "Iteration 8000\n",
            "Train set cost: 0.6621379769348825\n",
            "Test set cost: 0.677699890566014\n",
            "\n",
            "\n",
            "Iteration 8100\n",
            "Train set cost: 0.6619014148507533\n",
            "Test set cost: 0.6776587634701875\n",
            "\n",
            "\n",
            "Iteration 8200\n",
            "Train set cost: 0.6616686211412375\n",
            "Test set cost: 0.677621384260432\n",
            "\n",
            "\n",
            "Iteration 8300\n",
            "Train set cost: 0.6614395941365521\n",
            "Test set cost: 0.6775877496911344\n",
            "\n",
            "\n",
            "Iteration 8400\n",
            "Train set cost: 0.6612143320196925\n",
            "Test set cost: 0.6775578563710271\n",
            "\n",
            "\n",
            "Iteration 8500\n",
            "Train set cost: 0.6609928328267924\n",
            "Test set cost: 0.6775317007636482\n",
            "\n",
            "\n",
            "Iteration 8600\n",
            "Train set cost: 0.6607750944475026\n",
            "Test set cost: 0.6775092791878223\n",
            "\n",
            "\n",
            "Iteration 8700\n",
            "Train set cost: 0.6605611146253879\n",
            "Test set cost: 0.677490587818158\n",
            "\n",
            "\n",
            "Iteration 8800\n",
            "Train set cost: 0.6603508909583432\n",
            "Test set cost: 0.6774756226855644\n",
            "\n",
            "\n",
            "Iteration 8900\n",
            "Train set cost: 0.6601444208990283\n",
            "Test set cost: 0.6774643796777853\n",
            "\n",
            "\n",
            "Iteration 9000\n",
            "Train set cost: 0.6599417017553194\n",
            "Test set cost: 0.6774568545399516\n",
            "\n",
            "\n",
            "Iteration 9100\n",
            "Train set cost: 0.6597427306907807\n",
            "Test set cost: 0.6774530428751508\n",
            "\n",
            "\n",
            "Iteration 9200\n",
            "Train set cost: 0.6595475047251517\n",
            "Test set cost: 0.6774529401450147\n",
            "\n",
            "\n",
            "Iteration 9300\n",
            "Train set cost: 0.659356020734855\n",
            "Test set cost: 0.6774565416703242\n",
            "\n",
            "\n",
            "Iteration 9400\n",
            "Train set cost: 0.6591682754535193\n",
            "Test set cost: 0.6774638426316312\n",
            "\n",
            "\n",
            "Iteration 9500\n",
            "Train set cost: 0.6589842654725218\n",
            "Test set cost: 0.6774748380698984\n",
            "\n",
            "\n",
            "Iteration 9600\n",
            "Train set cost: 0.6588039872415461\n",
            "Test set cost: 0.6774895228871542\n",
            "\n",
            "\n",
            "Iteration 9700\n",
            "Train set cost: 0.6586274370691596\n",
            "Test set cost: 0.6775078918471671\n",
            "\n",
            "\n",
            "Iteration 9800\n",
            "Train set cost: 0.6584546111234066\n",
            "Test set cost: 0.6775299395761338\n",
            "\n",
            "\n",
            "Iteration 9900\n",
            "Train set cost: 0.6582855054324175\n",
            "Test set cost: 0.6775556605633856\n",
            "\n",
            "\n",
            "\n",
            "Final train set cost: 0.6581201158850365\n",
            "Final test set cost: 0.6775850491621099\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmrlGcpM6KEz"
      },
      "source": [
        "## Plotting Results\n",
        "\n",
        "Now, we plot our results to visualize how the model learned. In the following code box, we use matplotlib and the costs from the model training to create a graph with: \n",
        "\n",
        "$x$-axis: # of Iterations;\n",
        "\n",
        "$y$-axis: Cost."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "5rOHpq4h0MAe",
        "outputId": "1933e6f4-cc1d-4fcd-83da-2915fef44867"
      },
      "source": [
        "plt.plot(iteration_nums, costs_train)\n",
        "plt.plot(iteration_nums, costs_test)\n",
        "plt.title(\"# of Iterations v.s. Cost (Binary Crossentropy Cost)\")\n",
        "plt.xlabel(\"# of Iterations\")\n",
        "plt.ylabel(\"Cost (Binary Crossentropy Cost)\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEo4wxhQmTkP"
      },
      "source": [
        "## Making Predictions\n",
        "In the next code box, we make predictions for some value in the test set. We will print out the prediction along with the real value.\n",
        "\n",
        "Feel free to play around with this code and make predictions on different examples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dmD6UZxdmSjE",
        "outputId": "5317446d-1ac3-49e4-a526-3c3260dc093c"
      },
      "source": [
        "example_num = 10 # This is the example number. \n",
        "# Feel free to change it, but make sure that it is a non-negative number less than 114 to avoid indexing errors. \n",
        "hypothesis_example = hypothesis(X_test_np_norm[:,example_num], b, W)[0]\n",
        "real_label_example = y_test_np[:,example_num][0]\n",
        "print(\"Hypothesis: \" + str(round(hypothesis_example, 3)))\n",
        "print(\"Real Label: \" + str(round(real_label_example, 3)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hypothesis: 1.0\n",
            "Real Label: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mSIF7lYihR4"
      },
      "source": [
        "It turns out that the model only predicts 7 examples wrong out of 114 total examples in the test set. That's around 94% accuracy! \n",
        "\n",
        "For some applications, simple linear models will be sufficient, but many times, we wish our hypothesis function to be more complex, so we turn to Neural Networks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJAFl8fCCIVu"
      },
      "source": [
        "## Congratulations!\n",
        "You have now implemented a Logistic Regression model from scratch. You should be proud of yourself!\n",
        "\n",
        "The main takeaway from this assignment is the Gradient Descent algorithm, which has the following steps:\n",
        "\n",
        "1. Initialize Parameters\n",
        "2. Compute Hypothesis\n",
        "3. Compute Cost Function\n",
        "4. Update Parameters\n",
        "5. Repeat!\n",
        "\n",
        "I hope you had fun!"
      ]
    }
  ]
}